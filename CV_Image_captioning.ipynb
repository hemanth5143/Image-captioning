{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCYt2WtIkYxN",
        "outputId": "c15419f2-4d08-48c3-cd93-d24a44c054da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import string\n",
        "from pickle import dump\n",
        "from pickle import load\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, concatenate, add\n",
        "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout"
      ],
      "metadata": {
        "id": "AuS6pbp8kbBt"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the path to the image directory and caption file\n",
        "image_dir = '/content/drive/MyDrive/flickr8/Images'\n",
        "caption_file = '/content/drive/MyDrive/flickr8/captions.txt'"
      ],
      "metadata": {
        "id": "l0waHL2ykrBL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "def extract_features(directory):\n",
        "    base_model = ResNet50(weights='imagenet')\n",
        "    feature_extractor = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "    features = {}\n",
        "    for img in tqdm(os.listdir(image_dir)):\n",
        "        filename = os.path.join(image_dir, img)\n",
        "        image = load_img(filename, target_size=(224, 224))\n",
        "        image = img_to_array(image)\n",
        "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "        image = preprocess_input(image)\n",
        "        feature = feature_extractor.predict(image, verbose=0)\n",
        "        image_id = img.split('.')[0]\n",
        "        features[image_id] = feature\n",
        "    return features"
      ],
      "metadata": {
        "id": "8pgUEaT8ktrN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from images and save to pickle file\n",
        "dataset_images = '/content/drive/MyDrive/Flickr8k_Dataset/Images'\n",
        "features = extract_features(dataset_images)\n",
        "dump(features, open(\"features.p\", \"wb\"))\n",
        "\n",
        "# Load features from pickle file\n",
        "features = load(open(\"features.p\", \"rb\"))\n",
        "print('Extracted Features: %d' % len(features))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZv0jt1xkvli",
        "outputId": "9f98cbb7-7934-4668-dd6a-1134552b6ab9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels.h5\n",
            "102967424/102967424 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8091/8091 [14:05<00:00,  9.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Features: 8091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_shape = features['1000268201_693b08cb0e'].shape"
      ],
      "metadata": {
        "id": "NCwektsqk188"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "captions_file = open(caption_file, \"r\")\n",
        "FileContent = captions_file.read()\n",
        "captions_dict = {}\n",
        "for line in (FileContent.split('\\n')):\n",
        "    tokens = line.split(',')\n",
        "    image_id, caption = tokens[0], tokens[1:]\n",
        "    if len(line) < 2:\n",
        "      continue\n",
        "    image_id = image_id.split('.')[0]\n",
        "    caption = \" \".join(caption)\n",
        "    if image_id not in captions_dict:\n",
        "        captions_dict[image_id] = []\n",
        "    captions_dict[image_id].append('<start> ' + caption.strip() + ' <end>')\n",
        "del captions_dict[\"image\"]\n",
        "len(captions_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tf5YUSfk4uU",
        "outputId": "75036da9-ad93-4a9f-a6f7-eb1977ccf58a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8091"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_cleaning(captions_dict):\n",
        "    for key in captions_dict:\n",
        "        captions = captions_dict[key]\n",
        "        for i in range(len(captions)):\n",
        "            caption = captions[i]\n",
        "            caption = caption.lower()\n",
        "            cleaned_caption = \"\"\n",
        "            for char in caption:\n",
        "                if char.isalpha() or char == \" \":\n",
        "                    cleaned_caption += char\n",
        "            captions[i] = cleaned_caption"
      ],
      "metadata": {
        "id": "1SkQoBHCk6dX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_cleaning(captions_dict)\n",
        "captions = list(captions_dict.values())\n",
        "all_captions = [element for sublist in captions for element in sublist]\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(all_captions)\n",
        "vocab_size = tokenizer.num_words + 1 if tokenizer.num_words else len(tokenizer.word_index) + 1\n",
        "max_length = max(map(lambda caption: len(caption.split()), all_captions))"
      ],
      "metadata": {
        "id": "BlNANj1ol0N8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_ids = [*captions_dict]\n",
        "split = int(len(image_ids) * 0.75)\n",
        "train, test = image_ids[:split], image_ids[split:]"
      ],
      "metadata": {
        "id": "q16JmYHrk8EJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_generator(data_keys, captions_dict, features, tokenizer, max_length, vocab_size, batch_size):\n",
        "    while True:\n",
        "        X1, X2, y = [], [], []\n",
        "        n = 0\n",
        "        for key in data_keys:\n",
        "            n += 1\n",
        "            captions = captions_dict[key]\n",
        "            for caption in captions:\n",
        "                seq = tokenizer.texts_to_sequences([caption])[0]\n",
        "                for i in range(1, len(seq)):\n",
        "                  in_seq = pad_sequences([seq[:i]], maxlen=max_length)[0]\n",
        "                  out_seq = to_categorical([seq[i]], num_classes=vocab_size)[0]\n",
        "                  X1.append(features[key][0])\n",
        "                  X2.append(in_seq)\n",
        "                  y.append(out_seq)\n",
        "            if n == batch_size:\n",
        "              yield [np.array(X1), np.array(X2)], np.array(y)\n",
        "              X1, X2, y = [], [], []\n",
        "              n = 0"
      ],
      "metadata": {
        "id": "C-e0E-6NpgTx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enocder_input = feature_shape[1]\n",
        "embedd_dim = 150\n",
        "number_units = 256\n",
        "\n",
        "inputs1 = Input(shape=(enocder_input,))\n",
        "drop_out = Dropout(0.2)(inputs1)\n",
        "layer2 = Dense(number_units, activation='relu')(drop_out)\n",
        "\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "embed_layer = Embedding(vocab_size, embedd_dim, mask_zero=True)(inputs2)\n",
        "drop_out2 = Dropout(0.2)(embed_layer)\n",
        "layer_2 = LSTM(number_units)(drop_out2)\n",
        "decoder1 = add([layer2, layer_2])\n",
        "decoder2 = Dense(number_units, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ],
      "metadata": {
        "id": "zhkp1UtklCX2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "batch_size = 32\n",
        "steps = len(train) // batch_size\n",
        "\n",
        "for i in range(epochs):\n",
        "    # create data generator\n",
        "    generator = data_generator(train, captions_dict, features, tokenizer, max_length, vocab_size, batch_size)\n",
        "    # fit for one epoch\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pusXaedfoWoY",
        "outputId": "bb947127-d60b-40a0-f1e2-44722dcc74a3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "189/189 [==============================] - 87s 424ms/step - loss: 4.9837\n",
            "189/189 [==============================] - 63s 334ms/step - loss: 3.7080\n",
            "189/189 [==============================] - 63s 334ms/step - loss: 3.2637\n",
            "189/189 [==============================] - 62s 329ms/step - loss: 3.0117\n",
            "189/189 [==============================] - 63s 330ms/step - loss: 2.8363\n",
            "189/189 [==============================] - 63s 329ms/step - loss: 2.6975\n",
            "189/189 [==============================] - 63s 333ms/step - loss: 2.5847\n",
            "189/189 [==============================] - 63s 332ms/step - loss: 2.4917\n",
            "189/189 [==============================] - 62s 330ms/step - loss: 2.4134\n",
            "189/189 [==============================] - 63s 332ms/step - loss: 2.3404\n",
            "189/189 [==============================] - 62s 325ms/step - loss: 2.2801\n",
            "189/189 [==============================] - 63s 333ms/step - loss: 2.2280\n",
            "189/189 [==============================] - 63s 331ms/step - loss: 2.1783\n",
            "189/189 [==============================] - 64s 337ms/step - loss: 2.1313\n",
            "189/189 [==============================] - 63s 333ms/step - loss: 2.0865\n",
            "189/189 [==============================] - 63s 331ms/step - loss: 2.0453\n",
            "189/189 [==============================] - 64s 338ms/step - loss: 2.0052\n",
            "189/189 [==============================] - 64s 338ms/step - loss: 1.9774\n",
            "189/189 [==============================] - 63s 333ms/step - loss: 1.9467\n",
            "189/189 [==============================] - 65s 341ms/step - loss: 1.9171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_caption(model, image, tokenizer, max_length):\n",
        "    in_text = 'startseq'\n",
        "    for _ in range(max_length):\n",
        "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
        "        sequence = pad_sequences([sequence], max_length)\n",
        "        y_est = np.argmax(model.predict([image, sequence], verbose=0))\n",
        "        word = tokenizer.index_word.get(y_est)\n",
        "        if not word:\n",
        "            break\n",
        "        in_text += \" \" + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "\n",
        "    return in_text"
      ],
      "metadata": {
        "id": "g8WvFQf3s1H7"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "smoothie = SmoothingFunction().method1\n",
        "actual, predicted = [], []\n",
        "for i, sample in enumerate(test[:100]):\n",
        "    captions = captions_dict[sample]\n",
        "    y_pred = predict_caption(model, features[sample], tokenizer, max_length)\n",
        "    actual_captions = [caption.split() for caption in captions]\n",
        "    y_pred = y_pred.split()\n",
        "    actual.append(actual_captions)\n",
        "    predicted.append(y_pred)\n",
        "bleu_score = corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0), smoothing_function=smoothie)\n",
        "print(\"BLEU-1: %f\" % bleu_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4PKvyAjs9pK",
        "outputId": "6db7c63b-5991-4e36-c349-dc01071fadbf"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.131842\n"
          ]
        }
      ]
    }
  ]
}